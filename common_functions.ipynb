{"cells":[{"cell_type":"code","execution_count":3,"id":"6ab553d4","metadata":{"id":"6ab553d4","executionInfo":{"status":"ok","timestamp":1733857442117,"user_tz":-120,"elapsed":339,"user":{"displayName":"Ilias Iliopoulos","userId":"15741389353928327120"}}},"outputs":[],"source":["import copy, random\n","from collections import defaultdict\n","import numpy as np\n","from sklearn import pipeline\n","# from lime import lime_text\n","from sklearn.metrics import accuracy_score, classification_report, average_precision_score\n","import pandas as pd\n","\n","\n","def import_data(file_name_start, glob, pd, **kargs):\n","    file_list = glob(f'{file_name_start}*.txt')\n","    df_lst =[]\n","    for file in file_list:\n","        df_lst.append(pd.read_csv(file, **kargs))\n","    return pd.concat(df_lst).reset_index(drop=True)\n","\n","def call_spacy_nlp(nlp, x):\n","    return nlp(x)\n","\n","def build_tokens_list(doc):\n","    return [token.text for token in doc]\n","\n","def build_lemmas_list(doc):\n","    return [token.lemma_ for token in doc]\n","\n","def remove_not_alpha_list(doc):\n","    return [token.lemma_ for token in doc if token.lemma_.isalpha()]\n","\n","def process_data(vectorizer, X_train, X_test, y_train, y_test, suffix=''):\n","    X_train_bow = vectorizer.fit_transform(X_train['meas_action_comment_concat_3m_lemmas_string'])\n","    X_test_bow = vectorizer.transform(X_test['meas_action_comment_concat_3m_lemmas_string'])\n","\n","    dic_vocabulary = vectorizer.vocabulary_\n","    print('Vocabulary size:', len(dic_vocabulary))\n","\n","    X_train_bow_df_init = pd.DataFrame(X_train_bow.toarray())\n","    X_train_bow_df_init.columns = vectorizer.get_feature_names_out()\n","\n","    X_test_bow_df_init = pd.DataFrame(X_test_bow.toarray())\n","    X_test_bow_df_init.columns = vectorizer.get_feature_names_out()\n","\n","    drop_cols = ['meas_action_comment_concat_0', 'meas_action_comment_concat_-1',\n","                 'meas_action_comment_concat_-2', 'meas_action_comment_concat_3m',\n","                 'meas_action_comment_concat_3m_doc', 'meas_action_comment_concat_3m_lemmas',\n","                 'meas_action_comment_concat_3m_lemmas_string']\n","    X_train_reduced = X_train.drop(columns=drop_cols)\n","    X_train_bow_df = pd.concat([X_train_reduced.reset_index(drop=True), X_train_bow_df_init.reset_index(drop=True)], axis=1)\n","\n","    y_train = y_train.reset_index(drop=True)\n","\n","    X_test_reduced = X_test.drop(columns=drop_cols)\n","    X_test_bow_df = pd.concat([X_test_reduced.reset_index(drop=True), X_test_bow_df_init.reset_index(drop=True)], axis=1)\n","\n","    y_test = y_test.reset_index(drop=True)\n","\n","    X_train_bow_df_fin = X_train_bow_df.drop(columns=['account_id','snapnum'])\n","    X_test_bow_df_fin = X_test_bow_df.drop(columns=['account_id','snapnum'])\n","\n","\n","    return X_train_bow_df_fin, X_test_bow_df_fin, vectorizer, X_train_bow\n","\n","\n","def choose_separator(element1, element2):\n","    if element1 == '#' and element2 == '#':\n","        return ''\n","    elif element1 in ['[','xxxxÏ‚']:\n","        return ''\n","    else:\n","        return ' '\n","\n","def convert_list_to_string(x):\n","    new_lst = [choose_separator(x[i-1], x[i]) + el for i, el in enumerate(x)][1:]\n","    result = x[0] + ''.join(new_lst)\n","    return result\n","\n","\n","def find_index(lst, string):\n","    try:\n","        return next(i for i, tup in enumerate(lst) if tup[0] == string)\n","    except StopIteration:\n","        return -1\n","\n","\n","def model_eval(model, X_train, y_train, X_test, y_test, K=20):\n","    # Fit the model\n","    model.fit(X_train, y_train.values.ravel())\n","\n","    # Get predictions and probabilities\n","    predictions = model.predict(X_test)\n","    prob = model.predict_proba(X_test)\n","\n","    # Basic metrics\n","    accuracy = accuracy_score(y_test.values.ravel(), predictions)\n","    report = classification_report(y_test.values.ravel(), predictions, labels=[0, 1], target_names=[\"0\", \"1\"],\n","                                   output_dict=True)\n","    prec_1 = report[\"1\"][\"precision\"]\n","    rec_1 = report[\"1\"][\"recall\"]\n","    f1_1 = report[\"1\"][\"f1-score\"]\n","\n","    # Ranker metrics\n","    # Extract the positive class probabilities\n","    pos_prob = prob[:, 1]\n","\n","    # PR-AUC\n","    pr_auc = average_precision_score(y_test.values.ravel(), pos_prob)\n","\n","    # Sort indices by predicted probability in descending order\n","    sorted_indices = np.argsort(pos_prob)[::-1]\n","\n","    # Precision@K\n","    top_k_true = y_test.values.ravel()[sorted_indices[:K]]\n","    precision_at_k = np.mean(top_k_true) if K <= len(pos_prob) else np.nan\n","\n","    # Recall@K\n","    num_positives = np.sum(y_test.values.ravel())\n","    recall_at_k = (np.sum(top_k_true) / num_positives) if num_positives > 0 else 0.0\n","\n","    # R-Precision: Precision at R, where R = total number of positives\n","    if num_positives > 0:\n","        top_r_true = y_test.values.ravel()[sorted_indices[:num_positives]]\n","        r_precision = np.mean(top_r_true)\n","    else:\n","        r_precision = 0.0\n","\n","    # Return all metrics\n","    # Original returns: rec_1, prec_1, f1_1, accuracy, predictions, prob\n","    # Added: pr_auc, precision_at_k, recall_at_k, r_precision\n","    return rec_1, prec_1, f1_1, accuracy, pr_auc, precision_at_k, recall_at_k, r_precision\n","\n","\n","def eval_results(models, X_train, y_train, X_test, y_test, K=20):\n","\n","    results_bow = defaultdict(dict)\n","    models_to_train = copy.deepcopy(models)\n","\n","    for model in models_to_train:\n","        rec, prec, f1, acc, predictions, prob = model_eval(model[1], X_train, y_train, X_test, y_test)\n","\n","        # If prob is a 2D array, select the positive class probabilities\n","        if prob.ndim == 2 and prob.shape[1] == 2:\n","            prob = prob[:, 1]\n","\n","        # Compute PR-AUC\n","        pr_auc = average_precision_score(y_test, prob)\n","\n","        # For ranking-based metrics, sort by predicted probabilities (descending order)\n","        sorted_indices = np.argsort(prob)[::-1]\n","\n","        # Precision@K\n","        top_k_true = y_test[sorted_indices[:K]]\n","        precision_at_k = np.mean(top_k_true)  # fraction of positives in top K\n","\n","        # Recall@K\n","        num_positives = np.sum(y_test)\n","        recall_at_k = np.sum(top_k_true) / num_positives if num_positives > 0 else 0.0\n","\n","        # R-Precision\n","        top_r_true = y_test[sorted_indices[:num_positives]] if num_positives > 0 else []\n","        r_precision = np.mean(top_r_true) if num_positives > 0 else 0.0\n","\n","        results_bow[model[0]] = {\n","            'accuracy': acc,\n","            'recall': rec,\n","            'precision': prec,\n","            'f1_score': f1,\n","            'predictions': predictions,\n","            'probs': prob,\n","            f'precision_at_{K}': precision_at_k,\n","            f'recall_at_{K}': recall_at_k,\n","            'r_precision': r_precision,\n","            'pr_auc': pr_auc\n","        }\n","\n","    return results_bow, models_to_train\n","\n","\n","\n","# def lime_explainer(model_name, models, results, X_test, y_test, y_train, vectorizer, prob_threshold=0.7, num_features=3):\n","#     # find indices where both model and ground truth are positive\n","#     positive_indices =  [i for i, (x,y,z) in enumerate(zip(results[model_name]['predictions'].tolist(), results[model_name]['probs'][:,1].tolist(), y_test.values[:,0].tolist())) if (x == z == 1 and y > 0.7)]\n","#     negative_indices =  [i for i, (x,y,z) in enumerate(zip(results[model_name]['predictions'].tolist(), results[model_name]['probs'][:,0].tolist(), y_test.values[:,0].tolist())) if (x == z == 0 and y > 0.7)]\n","\n","#     ## select observation\n","#     i = random.choice(positive_indices) if len(positive_indices) > 0 else random.choice(negative_indices)\n","\n","#     model_index = find_index(models, model_name)\n","\n","#     model = pipeline.Pipeline([(\"vectorizer\", vectorizer),\n","#                                (\"classifier\", models[model_index][1])])\n","\n","#     txt_instance = X_test[\"meas_action_comment_concat_3m_lemmas_string\"].iloc[i]\n","\n","#     ## find explanation\n","#     explainer = lime_text.LimeTextExplainer(class_names=\n","#                  np.unique(y_train))\n","#     explained = explainer.explain_instance(txt_instance,\n","#                  model.predict_proba, num_features=3)\n","\n","#     return i, txt_instance, explained"]}],"metadata":{"kernelspec":{"display_name":"msc","language":"python","name":"msc"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}